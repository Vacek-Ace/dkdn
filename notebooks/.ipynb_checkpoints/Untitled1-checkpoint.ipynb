{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7749c54a-1f32-4264-8b5d-fc856b1b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fcb92c-cc0a-4356-bc3a-a1f5d17eb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from src.model.dkdn import *\n",
    "from src.model.instance_hardness import *\n",
    "from src.model.support_subset import *\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e826da80-af9d-422b-9669-1203b4c27486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: australian\n",
      "\n",
      "Heuristic thresholds computation ... \n",
      "Support subset estimation ... \n",
      "0.1\n",
      "0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Support subset estimation\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupport subset estimation ... \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m ss_idx, ini_performance \u001b[38;5;241m=\u001b[39m \u001b[43msampling_heuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplexity_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholds_ini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Incremental data evaluation\u001b[39;00m\n\u001b[1;32m     52\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_ini[ss_idx], y[ss_idx])\n",
      "File \u001b[0;32m~/Projects/general_retraining_ss/src/model/support_subset.py:77\u001b[0m, in \u001b[0;36msampling_heuristic\u001b[0;34m(complexity, X, y, clf, thresholds, eval_metric, random_state, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prop_sample)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# EstimaciÃ³n del support subset\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplexity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprop_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Entrenamiento de modelo en el SS\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X[idx], y[idx])\n",
      "File \u001b[0;32m~/Projects/general_retraining_ss/src/model/support_subset.py:33\u001b[0m, in \u001b[0;36mweighted_sample\u001b[0;34m(complexity, y, prop_sample, replace, distribution, random_state, kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(ids)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m samp \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m---> 33\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(samp \u001b[38;5;241m-\u001b[39m \u001b[43mcomplexity\u001b[49m\u001b[43m[\u001b[49m\u001b[43mids\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     34\u001b[0m     weights[np\u001b[38;5;241m.\u001b[39mwhere(arr \u001b[38;5;241m==\u001b[39m arr\u001b[38;5;241m.\u001b[39mmin())[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ns):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment = 'australian'\n",
    "print(f'Experiment: {experiment}\\n')\n",
    "\n",
    "results_folder = '../results/incremental'\n",
    "\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "data = pd.read_parquet(f'../data/{experiment}.parquet')\n",
    "\n",
    "exp_info = {experiment:{}}\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data.drop(columns=['y']))\n",
    "y = data.y.values\n",
    "y[y == -1] = 0\n",
    "y = y.astype(int)\n",
    "\n",
    "# Save test to evaluate models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "for incr in range(5, 10):\n",
    "# Incremental data\n",
    "incr = 0.2\n",
    "\n",
    "# Division initial set and incremental set\n",
    "X_ini, X_incr, y_ini, y_incr = train_test_split(X_train, y_train, test_size=incr, stratify=y_train, random_state=42)\n",
    "\n",
    "# random seed for random methods\n",
    "rng_seed = 1234\n",
    "\n",
    "# Heuristic thresholds\n",
    "print('Heuristic thresholds computation ... ')\n",
    "complexity_ini, higher_complexity_ini = complexity_high_class(X_ini, y_ini)\n",
    "thresholds_ini = expected_performance_thresholds(higher_complexity_ini)\n",
    "\n",
    "exp_info[experiment][incr] = {}\n",
    "\n",
    "# Read data info\n",
    "with open(f'../results/sampling/{experiment}.json', 'r') as fin:\n",
    "                exp_summary = json.load(fin)\n",
    "methods = [SVC, KNeighborsClassifier, RandomForestClassifier]\n",
    "\n",
    "for method in methods:\n",
    "    # Method setup\n",
    "    str_method = str(method())[:-2]\n",
    "    params = exp_summary[str_method]['best_params']\n",
    "    clf = method(**params)\n",
    "\n",
    "    # Support subset estimation\n",
    "    print('Support subset estimation ... ')\n",
    "    ss_idx, ini_performance = sampling_heuristic(complexity_ini, X_ini, y_ini, clf, thresholds_ini, random_state=rng_seed, verbose=True)\n",
    "\n",
    "    # Incremental data evaluation\n",
    "    clf.fit(X_ini[ss_idx], y[ss_idx])\n",
    "    pred_incr = clf.predict(X_incr)\n",
    "\n",
    "    # Incremental data sampling\n",
    "    if not (scaled_mcc(y_incr, pred_incr) > ini_performance) | (scaled_mcc(y_incr, pred_incr) > thresholds_ini[0]):\n",
    "        print('Incremental data thresholds computation ...')\n",
    "        complexity_incr, higher_complexity_incr = complexity_high_class(X_incr, y_incr)\n",
    "        thresholds_incr = expected_performance_thresholds(higher_complexity_incr)\n",
    "        print('Incremental sampling ...')\n",
    "        incr_idx, incr_performance = sampling_heuristic(complexity_incr, X_incr, y_incr, clf, thresholds_incr, random_state=rng_seed, verbose=True)\n",
    "        # New data to train model\n",
    "        X_new = np.append(X_ini[ss_idx], X_incr[incr_idx], axis=0)\n",
    "        y_new = np.append(y_ini[ss_idx], y_incr[incr_idx], axis=0)\n",
    "    else:\n",
    "        X_new = X_ini[ss_idx]\n",
    "        y_new = y_ini[ss_idx]\n",
    "\n",
    "    # Train new model\n",
    "    clf.fit(X_new, y_new)\n",
    "\n",
    "    # Performances computation\n",
    "    new_performance = scaled_mcc(y_new, clf.predict(X_new))\n",
    "    incr_performance = scaled_mcc(y_incr, clf.predict(X_incr))\n",
    "    test_performance = scaled_mcc(y_test, clf.predict(X_test))\n",
    "\n",
    "    method_info = {'proporcion': len(X_new)/len(X_train),\n",
    "        'test goal': exp_summary[str_method]['test_score'],\n",
    "        'test performance': test_performance,\n",
    "        'new performance': new_performance,\n",
    "        'ini performance': ini_performance,\n",
    "        'incr performance': incr_performance,\n",
    "        'ini thresholds': thresholds_ini,\n",
    "        'incr thresholds': thresholds_incr}\n",
    "    print(f'{str_method}: {method_info}')\n",
    "    exp_info[experiment][str_method] = method_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b25a873a-8b6c-423d-99ce-ec61b495a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ss] *",
   "language": "python",
   "name": "conda-env-ss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
